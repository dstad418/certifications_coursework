# certifications_coursework
Compiled coursework across all certifications.

### Google:
Data Engineering, Big Data, and Machine Learning on GCP Specialization
Cloud Database Engineer Specialization
Machine Learning Engineer Professional Certificate
Google Data Analytics
Google Advanced Data Analytics
Google Business Intelligence

### AWS:
AWS Fundamentals Certificate

### Microsoft: 
Azure Data Scientist Associate Certification (DP-100)
Azure Data Engineering Associate Certification (DP-203)


### Project Titles for each Associated Discipline:

### Software & Data Engineering
1. Performing an Online Database Migration
2. Migrating to Cloud SQL from Amazon RDS for MySQL Using Database Migration Service
3. Deploying Oracle on Bare Metal Solution
4. Migrating On-premises MySQL Using a Continuous Database Migration Service Job
5. Running SQL Server on Google Kubernetes Engine
6. Administering a Highly Available Cloud SQL for SQL Server Database
7. Creating SQL Server Databases on Compute Engine
8. Using Terraform to Create Clients and Servers
9. Use Terraform to Create a Network Peering
10. Using Terraform to Create Networks and Firewalls
11. Creating Cloud SQL Databases
12. Deploying a Modern Web App Connected to a Cloud Spanner Instance
13. Configuring Replication and Enabling Point-in-Time Recovery for Cloud SQL for PostgreSQL
14. Securing a Cloud SQL for PostgreSQL Instance
15. Connect an App to a Cloud SQL for PostgreSQL Instance
16. Migrate to Cloud SQL for PostgreSQL using Database Migration Service
17. Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL
18. Getting Started with VPC Networking and Google Compute Engine
19. Google Cloud Fundamentals: Getting Started with Cloud Marketplace
20. Cloud Spanner - Defining Schemas and Understanding Query Plans
21. Cloud Spanner - Loading Data and Performing Backups
22. Running Databases in GKE
23. Creating Databases in Compute Engine
24. Streaming Data Pipelines with Google Cloud Dataflow
25. Streaming Data Processing with Google Cloud Pub/Sub
26. Loading Data into BigQuery
27. Loading Taxi Data into Google Cloud SQL

### Data Science/Machine Learning/AI
1. TFX Metadata Exploration on Google Cloud
2. Running ML Pipelines on Vertex AI
3. BigQuery Integration in JupyterLab on Vertex AI
4. Unstructured Text Classification Using Natural Language API
5. Predict Bike Trip Duration with a Regression Model in BQML 2.5
6. Movie Recommendations in BigQuery ML 2.5
7. Predicting Penguin Weight using BigQuery ML
8. Introduction to Linear Regression with Python and Scikit-Learn
9. Exploratory Data Analysis Using Python and BigQuery
10. Entity and Sentiment Analysis with Google Cloud Natural Language API
11. Improving Data Quality
12. Predicting Loan Risk with Vertex AI AutoML
13. Leveraging Vertex AI Studio for Generative AI and Multimodal Analysis
14. Training and Deploying a TensorFlow Model in Vertex AI
15. Distributed Training with Keras on Google Cloud
16. Vertex AI: Training and Serving a Custom Model
17. Introduction to TensorFlow Data Validation
18. Training at Scale with Vertex AI Training Service
19. Building and Deploying a DNN using Keras on Vertex AI
20. Classifying Structured Data using Keras Preprocessing Layers
21. Utilizing the TensorFlow Dataset API
22. Structured Data Prediction using Vertex AI Platform
23. Performing Advanced Feature Engineering in Keras
24. Performing Basic Feature Engineering in BQML
25. Predicting Visitor Purchases with BigQuery ML
26. Building and Executing a Pipeline Graph with Data Fusion

### Data Analysis
1. Using BigQuery to do Analysis
2. Streaming Analytics and Dashboards with Looker Studio
3. Exploratory Data Analysis Using Python and BigQuery
4. Advanced Visualizations with TensorFlow Data Validation
5. TFX on Cloud AI Platform Pipelines
6. Exploring and Creating an Ecommerce Analytics Pipeline with Cloud Dataprep v1.5
7. Introduction to Linear Regression with Python and Scikit-Learn
8. Predicting Visitor Purchases with BigQuery ML
9. BigQuery Query Performance Optimization
10. Working with JSON and Array Data in BigQuery
11. Partitioned Tables in Google BigQuery
12. Loading Data into BigQuery
13. Using BigQuery to do Analysis
14. Real-Time Streaming Data Pipeline for Taxi Business Analytics
15. BigQuery Integration in JupyterLab on Vertex AI
16. Predicting Visitor Purchases with BigQuery ML
17. Cloud Spanner - Defining Schemas and Understanding Query Plans
18. Loading Taxi Data into Google Cloud SQL
19. Improving Data Quality
20. Streaming Data Pipelines with Google Cloud Dataflow
21. Streaming Data Processing with Google Cloud Pub/Sub
22. Building and Executing a Pipeline Graph with Data Fusion
23. Exploring and Creating an Ecommerce Analytics Pipeline with Cloud Dataprep v1.5
24. Entity and Sentiment Analysis with Google Cloud Natural Language API




### Skills:

### Software/Data Engineering
- **Database Migration:** SQL Server to Spanner, MySQL from Amazon RDS to Google Cloud SQL, on-premises MySQL to Cloud SQL, PostgreSQL using Database Migration Service
- **Cloud Platforms:** Google Cloud Platform (GCP), AWS, Oracle Bare Metal Solution
- **Database Management:** Oracle Database 19c, Microsoft SQL Server, MySQL, PostgreSQL, Cloud Spanner
- **Data Pipeline Tools:** Striim, Google Cloud Dataflow, Terraform, Cloud Data Fusion
- **Kubernetes:** Google Kubernetes Engine (GKE) deployment, Helm, Kubernetes secrets management
- **Data Verification:** Data integrity checks, replication configuration, point-in-time recovery, backup and restore operations
- **Networking:** VPC network peering, firewall configuration, IP allowlists, network security
- **Automation and Scripting:** Terraform, AWS CLI, Google Cloud CLI, Python scripting for automation
- **Containerization:** Docker, Google Cloud Run, custom container deployment on Vertex AI
- **Cloud Storage:** Google Cloud Storage bucket management, data export and import
- **Project Management:** End-to-end ML pipeline execution, environment setup, resource cleanup, project-specific configuration

### Data Science/Machine Learning/AI
- **Machine Learning and AI:** TensorFlow, Vertex AI, Kubeflow Pipelines, BigQuery ML, AutoML, AI Platform Notebooks
- **Feature Engineering:** Advanced feature engineering in Keras and BQML, temporal feature conversions, one-hot encoding
- **Model Training and Evaluation:** Distributed training with Keras, hyperparameter tuning, model monitoring, evaluation metrics (ROC AUC, Precision/Recall)
- **Natural Language Processing:** Google Cloud Natural Language API, text classification, sentiment analysis, entity extraction
- **Big Data:** BigQuery, partitioned tables, materialized views, Dataflow for large-scale data loading
- **Real-Time Data Processing:** Pub/Sub, streaming data ingestion, real-time analytics
- **Web Development:** Node.js, Flask-SQLAlchemy, Apache HTTP Server, PHP-MySQL integration
- **Containerization:** Docker, Google Cloud Run, custom container deployment on Vertex AI
- **Project Management:** End-to-end ML pipeline execution, environment setup, resource cleanup, project-specific configuration
  
### Data Analysis
- **Data Analysis:** Exploratory Data Analysis (EDA) with Python and BigQuery, SQL querying, JSON and ARRAY data handling
- **Data Verification:** Data integrity checks, replication configuration, point-in-time recovery, backup and restore operations
- **Data Visualization:** Looker Studio, Matplotlib, Seaborn, interactive dashboards
- **Natural Language Processing:** Google Cloud Natural Language API, text classification, sentiment analysis, entity extraction
- **Big Data:** BigQuery, partitioned tables, materialized views, Dataflow for large-scale data loading
- **Real-Time Data Processing:** Pub/Sub, streaming data ingestion, real-time analytics
- **Cloud Storage:** Google Cloud Storage bucket management, data export and import
- **Web Development:** Node.js, Flask-SQLAlchemy, Apache HTTP Server, PHP-MySQL integration




### Project Titles & Descriptions:

### Project: Performing an Online Database Migration
- **Database Migration:** Conducted an online migration of a SQL Server database to Spanner using Striim, ensuring continuous replication and minimal performance impact on the source database.
- **Striim Configuration:** Deployed Striim through Google Cloud Marketplace, configured SQL Server and Spanner connections, and set up continuous data pipelines.
- **Data Verification:** Verified data integrity post-migration by querying the Spanner database to confirm successful data transfer and replication.


### Project: Migrating to Cloud SQL from Amazon RDS for MySQL Using Database Migration Service
- **Database Migration:** Successfully migrated a MySQL database from Amazon RDS to Google Cloud SQL using Database Migration Service, ensuring data integrity and secure connectivity.
- **Cloud Configuration:** Configured AWS CLI in Google Cloud Shell, set up connection profiles, and managed IP allowlists for secure data transfer.
- **Verification and Testing:** Verified data integrity post-migration by connecting to Cloud SQL and executing SQL queries to confirm successful data transfer.


### Project: Deploying Oracle on Bare Metal Solution
- **Oracle Database Deployment:** Deployed Oracle Database 19c on a Compute Engine VM simulating a Bare Metal server, configured VPC network peering, and installed the database using Oracle toolkit.
- **Connectivity Configuration:** Established network peering between projects, verified database connectivity, and ensured secure data transfer between Oracle and Google Cloud services.
- **Vertex AI Integration:** Verified connectivity from Google Cloud Vertex AI, ran queries against the Oracle database, and visualized data using JupyterLab notebooks.


### Project: Migrating On-premises MySQL Using a Continuous Database Migration Service Job
- **Database Migration:** Successfully migrated an on-premises MySQL database to Cloud SQL for MySQL using a continuous Database Migration Service job, ensuring real-time data replication.
- **Connectivity Configuration:** Configured VPC peering for secure connectivity between the source and destination instances, and created connection profiles for seamless migration.
- **Data Verification and Promotion:** Verified data consistency post-migration, tested continuous data updates, and promoted the Cloud SQL instance to a standalone database for read/write operations.


### Project: Running SQL Server on Google Kubernetes Engine
- **Kubernetes Cluster Deployment:** Created a Kubernetes cluster in Google Kubernetes Engine (GKE) and configured it for SQL Server deployment.
- **SQL Server Configuration:** Deployed Microsoft SQL Server into the Kubernetes cluster, set up persistent storage, and managed Kubernetes secrets for secure database credentials.
- **Client Connectivity:** Connected to the SQL Server database from a client machine, verified database functionality, and ensured secure access using Kubernetes load balancer.


### Project: Administering a Highly Available Cloud SQL for SQL Server Database
- **High Availability Configuration:** Created a secure, highly available SQL Server database using Cloud SQL, with deployment across multiple zones and configured with private IP for secure access.
- **Backup and Restore Operations:** Performed backup and restore operations on the SQL Server database, including loading a sample database and restoring it from a backup.
- **Failover Testing:** Connected to the database using its private IP address and successfully triggered a failover to test high availability, ensuring seamless operation and data consistency.


### Project: Creating SQL Server Databases on Compute Engine
- **VM Provisioning:** Created client and server virtual machines (VMs) on Google Compute Engine, including a SQL Server database server in a private network and client machines in a public network for administration and database connectivity.
- **Database Administration:** Configured SQL Server, set up user authentication, and managed database access through Remote Desktop Protocol (RDP) and SQL Server Management Studio.
- **Firewall Configuration:** Established firewall rules to restrict SQL Server database access to the private network, ensuring secure database operations and connectivity from authorized clients.


### Project: Using Terraform to Create Clients and Servers
- **VM Provisioning with Terraform:** Created and configured Linux virtual machines for database server and client using Terraform, including a MySQL server in a private network and a client machine in a public network.
- **Database Server Configuration:** Set up MySQL on the server, configured remote access, and created user accounts for database administration and connectivity.
- **Client Connectivity:** Installed MySQL client software on the client machine and successfully connected to the MySQL server, demonstrating secure database operations and network isolation.


### Project: Use Terraform to Create a Network Peering
- **Network Peering:** Configured network peering between two GCP networks using Terraform, allowing internal IP communication. This setup facilitates secure database deployments in private networks.
- **VM Deployment:** Deployed VMs in both public and private networks, ensuring that the private VM had no external IP for enhanced security.
- **Verification and Connectivity:** Verified internal connectivity between peered networks by successfully pinging the private VM from the public VM, demonstrating the effectiveness of the network peering setup.


### Project: Using Terraform to Create Networks and Firewalls
- **Automated Network Infrastructure:** Designed and deployed a secure network infrastructure using Terraform, creating both public and private VPCs with appropriate subnet configurations.
- **Firewall Configuration:** Implemented firewall rules to control access, allowing SSH, RDP, and ICMP traffic as needed, ensuring secure communication between network components.
- **Virtual Machine Management:** Provisioned and managed multiple virtual machines in both public and private networks, facilitating a robust environment for database migration projects.


### Project: TFX Metadata Exploration on Google Cloud
- **AI Platform Pipelines Deployment:** Successfully created and configured an instance of AI Platform Pipelines, integrating Kubeflow Pipelines into Google Cloud for streamlined machine learning workflows.
- **Metadata Management:** Utilized the ML Metadata service to access and analyze pipeline artifacts stored in the AI Platform Pipelines instance, ensuring robust tracking and management of ML metadata.
- **End-to-End ML Pipeline Execution:** Cloned, configured, and executed example TFX pipelines using Vertex AI Workbench, demonstrating hands-on experience with cloud-based machine learning pipeline orchestration.


### Project: Creating Cloud SQL Databases
- **Cloud SQL Database Deployment:** Successfully created and configured both PostgreSQL and MySQL databases on Google Cloud SQL, leveraging Google Cloud's managed database services for high availability and performance.
- **Database Connectivity:** Established secure connections to Cloud SQL instances using the Google Cloud CLI and Cloud SDK, demonstrating proficiency in accessing and managing cloud-based databases.
- **Virtual Machine Integration:** Deployed a virtual machine as a client to connect and interact with the MySQL database, showcasing skills in integrating compute and database resources in a cloud environment.


### Project: Deploying a Modern Web App Connected to a Cloud Spanner Instance
- **Full-Stack Web Application Deployment:** Successfully deployed a Node.js application with both backend and frontend components using Docker and Google Cloud Run, ensuring scalable and serverless execution.
- **Cloud Spanner Integration:** Configured and connected the application to a Cloud Spanner instance for managing large-scale, horizontally scalable relational data with high availability.
- **Data Management and Simulation:** Imported sample stock trade data into Cloud Spanner and implemented data simulations within the application to visualize stock performance using Google Charts.


### Project: Configuring Replication and Enabling Point-in-Time Recovery for Cloud SQL for PostgreSQL
- **Point-in-Time Recovery Configuration:** Enabled point-in-time recovery for a Cloud SQL PostgreSQL instance to facilitate the recovery of databases to a specific state before data loss or errors occurred, ensuring data integrity and minimal downtime.
- **Replication Setup:** Configured and tested replication by performing a point-in-time recovery to create a new instance, verifying that data changes made after the specified point in time were not present in the recovered database.
- **Backup Management:** Enabled and managed automated backups for the Cloud SQL PostgreSQL instance, ensuring regular data snapshots for reliable recovery options.


### Project: Securing a Cloud SQL for PostgreSQL Instance
- **Customer-Managed Encryption Keys (CMEK):** Configured a Cloud SQL for PostgreSQL instance with customer-managed encryption keys to enhance data security by using custom cryptographic keys for data at rest.
- **pgAudit Configuration:** Enabled and configured pgAudit on the Cloud SQL instance to selectively record and track SQL operations, providing fine-grained control over database activity logging.
- **Cloud SQL IAM Database Authentication:** Set up and tested Cloud SQL IAM database authentication, allowing secure and granular access control to the PostgreSQL instance using Google Cloud IAM users.


### Project: Connect an App to a Cloud SQL for PostgreSQL Instance
- **Kubernetes Cluster Deployment:** Created and configured a Kubernetes cluster on Google Kubernetes Engine (GKE) and deployed a lightweight Flask-SQLAlchemy web application.
- **Database Connection Configuration:** Connected the GKE application to a Cloud SQL for PostgreSQL instance using Kubernetes secrets for secure database credentials management.
- **Full Read/Write Verification:** Verified the application's ability to write to and read from the Cloud SQL database, ensuring proper integration and functionality through application interactions and direct SQL queries.


### Project: Migrate to Cloud SQL for PostgreSQL using Database Migration Service
- **Prepared Source Database for Migration:** Configured a stand-alone PostgreSQL database by installing and enabling the pglogical extension, creating a migration user with necessary permissions, and setting up the database for migration.
- **Configured Continuous Migration:** Created and started a continuous Database Migration Service job, established VPC peering for secure connectivity, and ensured the source and destination databases were synchronized.
- **Verified and Promoted Cloud SQL Instance:** Confirmed successful data migration by querying the Cloud SQL instance, tested continuous data replication, and promoted the Cloud SQL instance to a stand-alone database for reading and writing operations.


### Project: Google Cloud Fundamentals: Getting Started with Cloud Storage and Cloud SQL
- **Deployed Web Server and Connected to Cloud SQL:** Created a VM instance (`bloghost`) with Apache, PHP, and PHP-MySQL, configured to connect to a Cloud SQL instance (`blog-db`) using provided credentials, and verified database connection success.
- **Created and Configured Cloud Storage Bucket:** Set up a Cloud Storage bucket named after the project ID, uploaded an image, set it to be publicly readable, and integrated this image into the web server's PHP script to display on the web page.


### Project: Getting Started with VPC Networking and Google Compute Engine
- **Explored and Configured VPC Networking:** Examined the default VPC network with subnets, routes, and firewall rules; deleted the default network; created a new auto mode VPC network (`mynetwork`) with similar configurations including subnets and firewall rules.
- **Created and Connected VM Instances:** Deployed two VM instances in different regions within the new VPC network and verified their connectivity using SSH and ICMP ping tests.
- **Tested and Modified Firewall Rules:** Evaluated the effects of firewall rules on VM connectivity by selectively removing ICMP and SSH rules and observing the resulting connectivity changes.


### Project: Google Cloud Fundamentals: Getting Started with Cloud Marketplace
- **Deploy LAMP Stack via Cloud Marketplace:** Used Google Cloud Marketplace to deploy the Bitnami LAMP stack on a Compute Engine instance, which provides a complete web development environment including Linux, Apache HTTP Server, MySQL, PHP, and phpMyAdmin.
- **Configure and Verify Deployment:** Configured the deployment settings, selected appropriate machine type and zone, and verified the deployment by accessing the site address to confirm the Apache HTTP Server is running.


### Project: Cloud Spanner - Defining Schemas and Understanding Query Plans
- **Load and Query Data:** Loaded data into tables (Portfolio, Category, Product) and used pre-built Python client libraries to load and query data in the Cloud Spanner instance.
- **Schema Updates and Secondary Indexes:** Added a new column (MarketingBudget) to the Category table, updated data, created secondary indexes, and performed queries using those indexes to optimize performance.
- **Examine Query Plans:** Explored Cloud Spanner query plans to understand execution plans for different types of queries, including aggregated and co-located join queries, examining the methods and rules by which Spanner creates and executes query plans.


### Project: Cloud Spanner - Loading Data and Performing Backups
- **Data Insertion Methods:** Inserted data into Cloud Spanner using DML, client libraries, and batch inserts to understand different data loading techniques and their efficiencies.
- **Dataflow for Large-Scale Data Loading:** Utilized Google Cloud Dataflow with a CSV file to load a large amount of data into Cloud Spanner efficiently, demonstrating the power of serverless, distributed data processing.
- **Database Backup:** Performed a Cloud Spanner database backup using the Cloud Console to ensure data availability and disaster recovery.


### Project: Running Databases in GKE
- **GKE Cluster Creation and MySQL Deployment:** Created a Google Kubernetes Engine (GKE) cluster and deployed a MySQL database using Kubernetes configuration files, including setting up Kubernetes secrets, PersistentVolumeClaim, Deployment, and Service resources.
- **Database Interaction and Management:** Accessed the MySQL database within the cluster, created a new database, and verified its creation to ensure proper deployment and functionality.
- **Helm Deployment:** Used Helm, a Kubernetes package manager, to deploy MySQL on the GKE cluster, demonstrating an alternative and efficient method for application deployment and management within Kubernetes.


### Project: Creating Databases in Compute Engine
- **Create a MySQL Database on Linux:** Set up a Compute Engine VM running Debian Linux, installed MySQL, secured the installation, and created a new database and table. Verified the setup by adding and querying data.
- **Create a SQL Server Database on Windows:** Deployed a SQL Server on a Windows Server 2019 Datacenter VM, configured SQL Server authentication, created a new database and table, and verified the setup by adding and querying data.
- **Automate Server Creation using Google Cloud SDK:** Used the gcloud CLI to automate the creation of a MySQL server on a Debian Linux VM with a startup script, and verified the server's functionality.


### Project: Movie Recommendations in BigQuery ML 2.5
- **Set Up Environment and Load MovieLens Data:** Initialize BigQuery environment, create a dataset, and load MovieLens data from Cloud Storage into BigQuery tables.
- **Explore Data and Evaluate Model:** Use SQL queries in BigQuery to explore the MovieLens dataset, verify data integrity, and evaluate a pre-trained collaborative filtering model using matrix factorization for movie recommendations.
- **Generate Recommendations and Perform Customer Targeting:** Use the trained model to generate movie recommendations for individual users, filter out movies already seen, and identify top users to target for specific movie promotions.
- **Batch Predictions:** Perform batch predictions for all user and movie combinations using BigQuery ML, facilitating large-scale recommendation generation without custom SQL queries for each user or movie.


### Project: Predict Bike Trip Duration with a Regression Model in BQML 2.5
- **Set Up Environment and Explore Bike Data:** Initialize BigQuery environment and explore the London bicycles dataset to identify and engineer features such as start station, day of the week, and hour of the day that influence trip duration.
- **Create Training Dataset and Build Initial Model:** Prepare a training dataset with relevant features and build a linear regression model in BigQuery ML. Evaluate the model's performance and refine it by combining days of the week into "weekday" and "weekend" categories and bucketizing the hour of the day to improve accuracy.
- **Make Predictions and Examine Model Weights:** Use the trained model with the TRANSFORM clause to make predictions on bike trip durations, both for single predictions and batch predictions. Extract and analyze model weights to understand the contribution of each feature to the prediction.


**Project Title: Streaming Data Pipelines with Google Cloud Dataflow**
- **Developed and deployed a Dataflow pipeline to process and analyze streaming traffic sensor data from Google Cloud Pub/Sub, integrating the processed data into BigQuery for real-time analytics.**
- **Implemented autoscaling in Dataflow to dynamically adjust compute resources, optimizing the processing of high-velocity data streams while maintaining low system lag and high throughput rates.**
- **Configured Cloud Monitoring to track key pipeline metrics, create alert policies, and build custom dashboards for real-time monitoring and operational insights.**


**Project Title: Streaming Data Processing with Google Cloud Pub/Sub**
- **Created and configured Pub/Sub topics and subscriptions to manage and process real-time traffic sensor data, ensuring reliable message delivery and system responsiveness.**
- **Developed and executed a Python-based sensor simulation script to generate and publish high-frequency traffic data streams to Pub/Sub, effectively simulating real-world data ingestion scenarios.**
- **Verified data integrity and message receipt using gcloud commands, ensuring seamless integration and message flow from Pub/Sub to downstream data processing pipelines.**


**Project Title: Running ML Pipelines on Vertex AI**
- **Configured and executed a Vertex AI Pipeline using the Kubeflow Pipeline SDK, showcasing integration of Python-based ML operations within a serverless framework.**
- **Set up environment permissions and storage configurations, utilizing Cloud Storage buckets for pipeline artifact management and ensuring secure access through IAM role assignments.**
- **Inspected and adjusted pipeline JSON configurations, deploying a pipeline to concatenate and reverse strings, and monitored execution through Vertex AI Pipelines dashboard and detailed job logs.**


**Project Title: BigQuery Integration in JupyterLab on Vertex AI**
- **Configured and launched a JupyterLab notebook instance on Vertex AI, selecting appropriate machine types and environment settings for optimal performance.**
- **Executed complex BigQuery SQL queries within the Jupyter notebook using the %%bigquery magic function, storing results in Pandas DataFrames for further data manipulation and analysis.**
- **Processed and visualized data by converting query outputs into Pandas Series objects and generating plots to illustrate relationships between flight departure and arrival delays, leveraging Pandas and Matplotlib libraries.**


**Project Title: Unstructured Text Classification Using Natural Language API**
- **Enabled Google Cloud Natural Language API and created an API key for authenticating requests to classify text using the classifyText method, handling a dataset with over 700 categories.**
- **Developed and executed a Python script leveraging Google Cloud client libraries to read text files from Cloud Storage, send them to the Natural Language API for classification, and store the categorized results in BigQuery for further analysis.**
- **Performed data analysis on the classified text data in BigQuery, using SQL queries to explore category distributions, filter results by confidence scores, and prepare the dataset for visualization and deeper insights.**


**Project Title: BigQuery Query Performance Optimization**
- **Implemented I/O minimization techniques in BigQuery by selectively querying necessary columns, reducing data read operations, and using efficient computations to enhance query performance.**
- **Utilized caching strategies and materialized views to store intermediate results, thereby speeding up query executions and avoiding repeated computations.**
- **Optimized join operations by denormalizing data, using window functions, and precomputing values to avoid large data shuffles and improve query performance significantly.**


**Project Title: Streaming Data Pipelines into Bigtable**
- **Developed a Dataflow pipeline to ingest real-time traffic sensor data from Pub/Sub and store it into Bigtable, enhancing data processing and storage capabilities.**
- **Configured and deployed a sensor data simulation script to continuously generate and publish traffic events to Pub/Sub, enabling consistent data flow for the pipeline.**
- **Utilized the HBase shell to query and validate the data stored in Bigtable, ensuring the accuracy and reliability of the streaming data pipeline.**


**Project Title: Streaming Analytics and Dashboards with Looker Studio**
- **Connected Looker Studio to BigQuery data source and created interactive visualizations to analyze traffic sensor data, enhancing real-time data analytics capabilities.**
- **Designed a bar chart using calculated fields to visualize vehicle counts per highway, improving the understanding of traffic patterns.**
- **Utilized custom SQL queries to generate detailed visualizations with metrics such as maximum, minimum, and average speeds per highway, facilitating deeper insights into traffic data trends.**


**Project Title: Building and Executing a Pipeline Graph with Data Fusion**
- **Created and configured a Cloud Data Fusion instance to build ETL/ELT data pipelines, demonstrating proficiency in cloud-based data integration and transformation.**
- **Loaded and cleaned NYC TLC Taxi Trips dataset, applied transformations using Wrangler, and joined data sources to enhance data quality and utility.**
- **Deployed and executed a data pipeline to process and store transformed data in BigQuery, ensuring efficient data flow and enabling advanced analytics.**


**Project Title: Migrating to Cloud SQL from Amazon RDS for MySQL Using Database Migration Service**
- **Configured and used Google Cloud Database Migration Service to migrate data from Amazon RDS for MySQL to Cloud SQL for MySQL, ensuring a seamless transition between cloud platforms.**
- **Installed and configured AWS CLI in Google Cloud Shell to manage AWS resources directly from Google Cloud, demonstrating cross-platform competency.**
- **Created a connection profile for the Amazon RDS instance and configured IP allowlists to ensure secure connectivity between source and destination databases.**
- **Successfully created, tested, and ran a one-time migration job, verifying data integrity by confirming the migrated data in Cloud SQL for MySQL.**


**Project Title: Cloud SQL for MySQL: Qwik Start**
- Successfully created and configured a Cloud SQL for MySQL instance in Google Cloud, ensuring secure and efficient database setup.
- Connected to the Cloud SQL instance using the mysql client in Cloud Shell, demonstrating proficiency in cloud-based database management.
- Designed and populated a SQL database with sample data, showcasing skills in SQL operations and data management within a cloud environment.


**Project Title: Running Apache Spark Jobs on Cloud Dataproc**
- Successfully migrated existing Apache Spark jobs to Cloud Dataproc, demonstrating proficiency in transitioning workloads to Google Cloud.
- Modified Spark jobs to utilize Google Cloud Storage instead of HDFS, optimizing the separation of compute and storage for enhanced performance.
- Automated Spark job deployment on job-specific clusters, ensuring efficient resource utilization and streamlined job execution on Google Cloud.


**Project Title: Partitioned Tables in Google BigQuery**
- Created and managed date-partitioned tables in Google BigQuery to optimize query performance and reduce costs by minimizing data processed.
- Implemented auto-expiring partitioned tables to comply with data privacy regulations and prevent unnecessary storage costs.
- Conducted advanced SQL queries on partitioned datasets to extract meaningful insights from large-scale ecommerce data and public datasets.


**Project Title: Working with JSON and Array Data in BigQuery**
- Ingested and managed semi-structured JSON data in Google BigQuery, leveraging the platform's native support for JSON, ARRAY, and STRUCT data types.
- Utilized SQL functions like ARRAY_AGG() and UNNEST() to efficiently query and manipulate nested and repeated fields, optimizing performance and simplifying data analysis.
- Implemented advanced querying techniques on complex datasets, extracting meaningful insights and improving data reporting capabilities.


**Project Title: Loading Data into BigQuery**
- Created and managed datasets in Google BigQuery, including ingestion of data from various sources such as local CSV files and Google Cloud Storage, using both the BigQuery Console and CLI.
- Applied Data Definition Language (DDL) to create and manipulate tables, enabling efficient data organization and retrieval.
- Executed SQL queries to analyze ingested data, extracting meaningful insights and validating successful data loading and table creation.


**Project Title: Loading Taxi Data into Google Cloud SQL**
- Created and configured a Cloud SQL instance, including setting up root access and whitelisting IP addresses for secure management.
- Imported NYC taxi trip data from CSV files into the Cloud SQL database, ensuring proper schema creation and data integrity through structured queries.
- Conducted data validation and integrity checks on imported data, identifying potential anomalies and ensuring the accuracy and reliability of the dataset.


**Project Title: Using BigQuery to do Analysis**
- Conducted ad-hoc interactive queries on BigQuery console to analyze New York City bicycle rental data and weather data from NOAA.
- Combined datasets using SQL queries to derive insights, such as calculating typical trip duration and total distance traveled by bicycles.
- Determined correlation between weather conditions and bicycle rentals, finding that bicycle rentals decrease by 47% on rainy days.


**Project Title: TFX on Cloud AI Platform Pipelines**
- Deployed Kubeflow Pipelines on Google Kubernetes Engine, integrating it with Google Cloud as AI Platform Pipelines for orchestrating TFX pipelines.
- Utilized AI Platform Notebooks for experimentation and development, cloning the example repository and running Jupyter notebooks to configure the TFX pipeline environment.
- Built, deployed, and monitored TensorFlow Extended (TFX) pipelines using the TFX CLI, executing cloud-based training jobs and leveraging AI Platform services for model training, tuning, and prediction.


**Project Title: TFX Standard Components Walkthrough**
- **Developed a high-level understanding of TFX pipeline components**: Utilized TensorFlow Extended (TFX) for analyzing, preprocessing, and training a multi-class classification model on the Covertype dataset, understanding key TFX components like CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer, and Evaluator.
- **Prototyped TFX pipelines using TFX Interactive Context**: Used Jupyter Notebooks within AI Platform Notebooks to create and run TFX pipelines interactively, leveraging TensorFlow Data Validation (TFDV) for data analysis and TensorFlow Transform (TFT) for scalable preprocessing.
- **Deployed and monitored TFX pipelines on AI Platform Pipelines**: Created and managed AI Platform Pipelines (Kubeflow Pipelines) on Google Kubernetes Engine (GKE), deploying TFX components to the cloud, and ensuring proper model evaluation with TensorFlow Model Analysis (TFMA).


**Project Title: Training and Deploying a TensorFlow Model in Vertex AI**
- **Trained a TensorFlow model locally using Vertex AI Workbench**: Utilized Vertex AI Workbench for data processing and exploratory data analysis with BigQuery, and developed a TensorFlow Regressor model to predict customer lifetime value.
- **Containerized training code and ran Vertex AI custom training job**: Created a managed Tabular dataset for experiment tracking, containerized the training code using Cloud Build, and deployed the container to Google Cloud Artifact Registry to run a Vertex AI custom training job.
- **Deployed the model to Vertex Online Prediction Endpoint**: Used Vertex TensorBoard to visualize model performance, deployed the trained model to a Vertex Online Prediction Endpoint for serving predictions, and made online predictions with explanations to understand model behavior.


**Project Title: TPU Speed Data Pipelines**
- **Used the tf.data.Dataset API to load training data**: Leveraged the tf.data.Dataset API to efficiently load and preprocess training data, ensuring a consistent data pipeline that matches the high-speed requirements of TPUs.
- **Implemented TFRecord format for efficient data loading from Cloud Storage**: Utilized TFRecord format to optimize data storage and loading from Cloud Storage, significantly improving the performance and scalability of the data pipeline for TPU training.
- **Executed data pipelines in Vertex AI Notebooks**: Launched and managed Vertex AI Notebooks to develop, test, and run the data pipelines, ensuring smooth integration and operation within the Google Cloud environment.


**Project Title: Distributed Training with Keras on Google Cloud**
- **Implemented Distributed Training:** Utilized the `tf.distribute.Strategy` API to perform synchronous training on multiple GPUs, enhancing model training efficiency and scalability.
- **Vertex AI Integration:** Launched and managed Vertex AI Notebooks, enabling an environment for developing, testing, and deploying machine learning models on Google Cloud.
- **Model Development and Deployment:** Developed a Keras model, defined callbacks, and executed distributed training workflows, culminating in the evaluation and deployment of the model in a cloud-based setup.


**Project Title: Vertex AI: Training and Serving a Custom Model**
- **Containerized Model Training:** Developed and containerized TensorFlow model training code using Docker, and pushed the container to Google Container Registry for scalable and efficient model training.
- **Custom Training on Vertex AI:** Deployed the custom container to Vertex AI for running a managed training job, leveraging Vertex AI's infrastructure to streamline the model training process.
- **Model Deployment and Prediction:** Deployed the trained model to a Vertex AI endpoint for serving predictions, utilizing the Vertex AI API to obtain predictions from the deployed model in a seamless and efficient manner.


**Project Title: Advanced Visualizations with TensorFlow Data Validation**
- **Data Investigation and Visualization:** Utilized TensorFlow Data Validation (TFDV) to compute and visualize descriptive statistics, infer schemas, and identify anomalies within datasets to ensure data quality and consistency.
- **Error Detection and Correction:** Conducted thorough evaluations of the data to detect errors and anomalies, implemented fixes, and ensured data alignment across training, evaluation, and serving datasets.
- **Drift and Skew Analysis:** Performed drift and skew analysis to monitor dataset changes over time, ensuring the robustness of the production pipeline and maintaining the integrity of the data.


**Project Title: Introduction to TensorFlow Data Validation**
- **Statistics Generation and Visualization:** Employed TensorFlow Data Validation (TFDV) to generate and visualize comprehensive statistics on datasets, enabling a deep understanding of feature distributions and identifying potential anomalies.
- **Schema Inference and Updating:** Utilized TFDV to automatically infer schemas from datasets and update them to ensure consistency and accuracy across different datasets, including training and inference datasets.
- **Data Comparison and Validation:** Leveraged TFDV's visualization tools to compare datasets, monitor changes over time, and validate data integrity, ensuring robust and reliable machine learning pipelines.


**Project Title: Structured Data Prediction using Vertex AI Platform**
- **Model Development and Deployment:** Trained, evaluated, and deployed a machine learning model to predict baby weight using Vertex AI notebooks, ensuring an efficient end-to-end machine learning workflow.
- **Data Management:** Created and managed a BigQuery dataset and Cloud Storage bucket, enabling seamless data export from BigQuery to CSV files stored in Cloud Storage.
- **Integration and Execution:** Cloned course repositories within Vertex AI notebooks and executed structured data prediction workflows, leveraging the latest TensorFlow Enterprise environment for optimal performance.


**Project Title: Running Pipelines on Vertex AI 2.5**
- **Project Environment Setup:** Configured project environment including necessary IAM roles and created Cloud Storage buckets to support AI pipeline execution on Vertex AI.
- **Pipeline Configuration and Inspection:** Updated and inspected a pre-defined AI pipeline JSON file, ensuring correct project-specific configurations and understanding of the pipeline's operations.
- **Pipeline Execution:** Successfully executed a simple Kubeflow Pipeline SDK-derived ML pipeline on Vertex AI, demonstrating the orchestration and execution of ML workflows in a serverless framework.


**Project Title: Introduction to Vertex Pipelines**
- **Cloud Storage and Environment Setup:** Created a Cloud Storage bucket and configured necessary permissions for Vertex AI Pipelines. Enabled required APIs and set up the project environment in Google Cloud.
- **Launching and Configuring Vertex AI Notebooks:** Launched a Vertex AI Workbench notebook, configured it with TensorFlow Enterprise 2.11, and cloned the course repository for further use.
- **Building and Running Vertex Pipelines:** Built and executed a 3-step introductory ML pipeline using the Kubeflow Pipelines SDK, demonstrating scalable pipeline creation and management on Vertex AI.


**Project Title: Monitoring Vertex AI Models**
- **Environment Setup and API Configuration:** Enabled necessary APIs including Notebooks API and Vertex AI API. Launched and configured Vertex AI Workbench notebook with TensorFlow Enterprise 2.11.
- **Model Monitoring Configuration:** Deployed a pre-trained model, configured model monitoring, and generated artificial traffic to evaluate model performance in production.
- **Analysis and Cleanup:** Interpreted the data reported by the model monitoring feature and cleaned up resources by undeploying the model and deleting endpoints and models from the Vertex AI console.


**Project Title: Hyperparameter Tuning with Vertex AI**
- **Environment Setup and Code Containerization:** Configured the Google Cloud environment by enabling necessary APIs and launching a Vertex AI Notebooks instance. Created and containerized the TensorFlow model training application for hyperparameter tuning.
- **Hyperparameter Tuning Job Execution:** Deployed the custom container to Google Container Registry and initiated a hyperparameter tuning job on Vertex AI, configuring multiple hyperparameters such as learning rate, momentum, and number of neurons.
- **Results Evaluation and Resource Cleanup:** Monitored the hyperparameter tuning job, analyzed the results of multiple training trials, and cleaned up resources including the notebook instance and storage bucket.


**Project Title: Vertex AI Workbench Notebook: Quick Start**
- **Environment Setup and Repository Cloning:** Launched a Vertex AI Workbench notebook, configured the environment with TensorFlow Enterprise 2.11, and cloned the training-data-analyst repository to the JupyterLab instance.
- **Model Training and Deployment:** Trained a TensorFlow 2.x classification model on the United States Census Income Dataset both locally and on the cloud using Vertex AI, created a Vertex AI model, and deployed a version for online prediction.
- **Model Serving and Prediction:** Successfully tested the deployed model by requesting online predictions, verifying the model's capability to classify income categories based on census data.
  

**Project Title: Exploring and Creating an Ecommerce Analytics Pipeline with Cloud Dataprep v1.5**
- **Data Preparation and Connection:** Connected BigQuery datasets to Cloud Dataprep, created an empty BigQuery dataset to receive the output table, and copied a subset of the public raw ecommerce dataset for exploration and cleaning in Cloud Dataprep.
- **Data Exploration and Cleaning:** Used Cloud Dataprep to explore dataset quality, removed duplicates, deleted unused columns, created calculated fields, filtered rows, and enriched data by creating a unique session ID and mapping eCommerce action types to labels.
- **Pipeline Creation and Scheduling:** Built a data transformation pipeline in Cloud Dataprep, scheduled transformation jobs, and output results to BigQuery, ensuring the pipeline runs at a scheduled interval to provide updated analytics for ecommerce transactions.


**Project Title: Performing Advanced Feature Engineering in Keras**
- **Environment Setup and Initialization:** Launched Vertex AI Workbench notebooks and cloned the course repository to set up the environment for developing and training a machine learning model.
- **Feature Engineering with Keras:** Built a taxifare price prediction model using Keras, incorporating advanced feature engineering techniques such as processing temporal features, using Lambda layers for geolocation feature engineering, and creating bucketized and crossed feature columns.
- **Model Training and Execution:** Executed the training of the taxifare prediction model, utilizing the engineered features to improve the accuracy of fare amount predictions for NYC taxi cab rides.


**Project Title: Performing Basic Feature Engineering in BQML**
- **Environment Setup:** Enabled all necessary APIs, including BigQuery and Vertex AI, and launched a Vertex AI Workbench notebook instance. Cloned the required course repository within the Vertex AI Notebooks instance to set up the environment for feature engineering tasks.
- **Feature Engineering with BQML:** Utilized BigQuery ML (BQML) to perform feature engineering on a taxi fare prediction model for NYC taxi rides. This included creating SQL statements to evaluate the model, extracting temporal features, and performing feature crosses on these temporal features.
- **Model Improvement:** Enhanced the fare amount prediction model through advanced feature engineering techniques, leading to better model accuracy and performance.
  

**Project Title: Training at Scale with Vertex AI Training Service**
- **Environment Setup:** Enabled necessary APIs including AI Platform Training & Prediction API and Vertex AI API. Created a Cloud Storage bucket for storing model artifacts and other data. Launched a Vertex AI Notebooks instance and cloned the course repository within the Vertex AI Notebooks instance.
- **Organizing Training Code:** Structured the training code into a Python package, preparing it for cloud-based training. This organization facilitates better code management and scalability.
- **Cloud Training Execution:** Used Google Cloud's Vertex AI Training Service to train the model on cloud infrastructure. This included running the training package and utilizing Docker containers to push training Docker images to a Docker registry, ensuring efficient and scalable model training.


**Project Title: Build a DNN using the Keras Functional API**
- **Environment Setup:** Enabled all recommended APIs and launched a Vertex AI Workbench notebook. Cloned the necessary course repository within the Vertex AI Notebooks instance.
- **Model Development:** Built a Deep Neural Network (DNN) using the Keras Functional API to predict the fare amount for NYC taxi cab rides. The tasks included reading in CSV file data using `tf.data`, specifying the input, hidden, and output layers of the DNN, and reviewing and visualizing the final DNN architecture.
- **Model Training and Deployment:** Trained the model locally and visualized the loss curves to monitor training performance. Deployed the trained model using the Cloud AI Platform and performed predictions to evaluate model accuracy and performance.


**Project Title: Building and Deploying a DNN using Keras on Vertex AI**
- **Developed a Deep Neural Network (DNN):**
  - Built and trained a DNN model using the Keras Sequential API to predict taxi fare amounts based on the NYC taxi dataset.
- **Configured and Utilized Vertex AI:**
  - Set up and managed a Vertex AI Workbench notebook for model development.
  - Cloned necessary repositories and configured the environment for optimal performance.
- **Model Deployment and Prediction:**
  - Deployed the trained model on Google Cloud's Vertex AI for real-time predictions.
  - Implemented online prediction services to validate the model's performance in a production environment.


**Project Title: Classifying Structured Data using Keras Preprocessing Layers**
- **Data Processing and Pipeline Construction:**
  - Loaded and preprocessed structured data from CSV files using Pandas and tf.data for efficient batching and shuffling.
- **Model Development with Keras:**
  - Employed Keras preprocessing layers to map CSV columns to features, and built a deep learning model using the Keras Sequential API.
- **Model Training and Evaluation:**
  - Trained the model using Keras, evaluated its performance, and applied the model to classify structured data, leveraging Vertex AI for scalable computation.


**Project Title: Utilizing the TensorFlow Dataset API**
- **Data Ingestion and Preprocessing:**
  - Implemented data ingestion from memory and disk using the TensorFlow Dataset API for efficient data manipulation and feature engineering.
- **Training Loop Integration:**
  - Integrated tf.data with a training loop to perform stochastic gradient descent, ensuring efficient data batching, shuffling, and preprocessing for model training.
- **Production Input Pipelines:**
  - Developed and optimized production-ready input pipelines with advanced feature engineering techniques to enhance model performance and scalability.


**Project Title: Predicting Penguin Weight using BigQuery ML**
- **Model Creation and Training:**
  - Developed a linear regression model in BigQuery ML to predict penguin weight based on species, island of residence, culmen length and depth, flipper length, and sex.
- **Model Evaluation and Prediction:**
  - Evaluated model performance using ML.EVALUATE function and made predictions using ML.PREDICT function, ensuring accurate results by understanding key metrics like mean squared error and R2 score.
- **Explainability and Feature Attribution:**
  - Applied explainable AI methods with ML.EXPLAIN_PREDICT to understand model predictions and identify significant features, enhancing the transparency and interpretability of the machine learning model.


**Project Title: Introduction to Linear Regression with Python and Scikit-Learn**
- **Data Analysis and Visualization:**
  - Analyzed a Pandas DataFrame and created exploratory data analysis plots using Seaborn to understand the housing dataset features and their relationships.
- **Model Training:**
  - Trained a linear regression model using Scikit-Learn to predict housing prices, applying foundational machine learning techniques.
- **Model Evaluation:**
  - Evaluated the performance of the linear regression model, ensuring its effectiveness in predicting housing prices, laying the groundwork for more complex algorithms and models.


**Project Title: Exploratory Data Analysis Using Python and BigQuery**
- **Data Analysis and Visualization:**
  - Analyzed a Pandas DataFrame and created exploratory data analysis plots using Seaborn to understand the dataset features and their relationships.
- **SQL Querying with BigQuery:**
  - Wrote and executed SQL queries to extract specific fields from a BigQuery dataset for further analysis.
- **Integrating Python with BigQuery:**
  - Conducted exploratory analysis using Python and BigQuery, combining the power of SQL with Python's data manipulation and visualization capabilities.


**Project Title: Improving Data Quality**
- **Data Cleaning and Preprocessing:**
  - Resolved missing values and corrected wrong data, ensuring the dataset is ready for machine learning algorithms.
- **Feature Engineering:**
  - Converted date features to datetime format, renamed feature columns, removed unnecessary values, and created one-hot encoding features for categorical data.
- **Temporal Feature Conversions:**
  - Understood and implemented temporal feature conversions to enhance the predictive power of the machine learning models.


**Project Title: Leveraging Vertex AI Studio for Generative AI and Multimodal Analysis**
- **Implemented Multimodal AI:** Analyzed and processed images using Gemini multimodal, generating titles, descriptions, and extracting text with tailored prompts to enhance image content understanding.
- **Designed Custom Prompts:** Developed and tested custom prompts using free-form and structured modes in Vertex AI Studio, employing zero-shot, one-shot, and few-shot prompting methods to achieve precise model outputs.
- **Developed Conversational AI:** Created context-aware conversational models, guiding interactions based on predefined contexts and constraints, simulating real-world support scenarios for enhanced user experiences.


**Project Title: Predicting Loan Risk with Vertex AI AutoML**
- **Dataset Preparation and Upload:** Successfully uploaded a tabular dataset to Vertex AI, ensuring it was ready for machine learning model training to predict loan risk.
- **Model Training and Evaluation:** Utilized Vertex AI's AutoML capabilities to train a classification model, followed by evaluating the model performance using key metrics like Precision/Recall curve, Confusion Matrix, and Feature Importance.
- **Model Deployment and Predictions:** Deployed the trained model to an endpoint and used it to generate predictions on new data, demonstrating the ability to integrate the model into real-world applications for loan risk prediction.


**Project Title: Predicting Loan Risk with Vertex AI AutoML**
- **Data Preparation and Upload:** Uploaded a tabular dataset to Vertex AI and generated descriptive statistics to prepare the data for training a machine learning model.
- **Model Training and Evaluation:** Trained a classification model using Vertex AI AutoML, evaluated model performance through metrics such as Precision/Recall curve, Confusion Matrix, and Feature Importance.
- **Deployment and Prediction:** Deployed the trained model to an endpoint, retrieved predictions using the Vertex AI API, and tested various input scenarios to ensure accurate loan risk predictions.


**Project Title: Entity and Sentiment Analysis with Google Cloud Natural Language API**
- **API Key Creation and Request Setup:** Generated an API key and constructed API requests using JSON to analyze text data using Google Cloud Natural Language API.
- **Entity and Sentiment Analysis:** Extracted entities and performed sentiment analysis on text to identify key components and their sentiment scores, including detailed entity sentiment breakdown.
- **Multilingual Processing and Syntax Analysis:** Analyzed text syntax and parts of speech, and created API requests in different languages to demonstrate the Natural Language API's multilingual capabilities.


**Project Title: Predicting Visitor Purchases with BigQuery ML**
- **Explored and Analyzed Ecommerce Data:** Utilized Google Analytics data to determine purchasing patterns and behaviors, identifying top-selling products and calculating the percentage of visitors making purchases.
- **Developed and Trained Machine Learning Models:** Created logistic regression models in BigQuery ML to predict visitor purchase likelihood, improving model performance through feature engineering and evaluating model effectiveness with metrics such as ROC AUC.
- **Predicted Purchase Probabilities and Enhanced Marketing ROI:** Used the trained model to predict the probability of first-time visitors making future purchases, identifying high-value visitors for targeted marketing campaigns and significantly increasing marketing ROI by 9x.


**Project Title: Predicting Loan Risk with Vertex AI AutoML**
- **Developed and Deployed Predictive Model:** Utilized Vertex AI to upload a dataset, train a machine learning model with AutoML, and deploy the model to an endpoint for predicting loan repayment risk.
- **Evaluated Model Performance:** Assessed the model using metrics such as Precision/Recall curve, Confusion Matrix, and Feature Importance to ensure accuracy and reliability in predicting loan defaults.
- **Generated Predictions:** Implemented predictions using the trained model, leveraging environment variables and the SML service to evaluate loan risk for various customer scenarios, improving decision-making in loan approvals.


**Project Title: Predicting Visitor Purchases with BigQuery ML**
- **Utilized BigQuery for Data Analysis and Model Training:** Explored and queried the Google Analytics Sample Ecommerce dataset using BigQuery to understand customer purchasing habits. Created training and evaluation datasets to predict visitor purchase behavior with minimal coding.
- **Developed and Evaluated Machine Learning Models:** Created logistic regression and boosted tree classifier models in BigQuery ML. Evaluated model performance using ROC AUC and other metrics, achieving a significant improvement in predictive power through feature engineering.
- **Generated Purchase Predictions:** Predicted the probability of first-time visitors making purchases in subsequent visits, providing actionable insights to enhance marketing ROI by targeting high-value users.


### Project Title: Real-Time Streaming Data Pipeline for Taxi Business Analytics
- **Developed a Real-Time Data Pipeline:** Utilized Google Cloud Dataflow to stream and process real-time NYC taxi data, integrating with BigQuery for efficient data storage and analysis.
- **Performed Data Analysis:** Conducted SQL queries in BigQuery to analyze real-time taxi ride metrics, including revenue, passenger counts, and ride statuses, ensuring comprehensive data insights.
- **Created Real-Time Dashboards:** Designed and implemented interactive dashboards in Looker Studio, visualizing key business metrics for real-time monitoring and strategic decision-making.


### Project Title: Comprehensive Data Science and Machine Learning Projects Using Python and Azure
- **Developed Machine Learning Models:** Utilized Python libraries (NumPy, Pandas, scikit-learn, PyTorch, TensorFlow) to explore, visualize, and manipulate data, and created regression, classification, and clustering models for predictive analytics.
- **Implemented Azure Machine Learning Pipelines:** Built and deployed machine learning models using Azure ML services, including automated machine learning, model evaluation, real-time predictions, and data transformations.
- **Streamlined Data Processing and Visualization:** Established a real-time data pipeline with Google Cloud Dataflow for streaming data into BigQuery and created interactive dashboards using Looker Studio for comprehensive data analysis and visualization.


### Project Title: Designing and Implementing Data Storage and Processing Solutions with Azure
- **Architected Data Storage Solutions:** Designed efficient data storage structures, including Azure Data Lake solutions, file types for storage and analytical queries, partition strategies, and star schemas for optimal querying and data transformation.
- **Implemented Data Processing Pipelines:** Developed data ingestion and transformation pipelines using Apache Spark, Azure Data Factory, Synapse Pipelines, and Stream Analytics, handling tasks such as data cleansing, normalization, and incremental data loads.
- **Ensured Data Security and Performance:** Designed and implemented robust data security measures including data encryption, masking, and auditing strategies, while optimizing data storage and processing performance through monitoring, logging, and tuning query performance.


### Project Title: Training and Evaluating Machine Learning Models with Azure and Jupyter
- **Trained Clustering Model:** Utilized Azure Machine Learning Workspace to develop and evaluate a clustering model, focusing on separating training cases based on feature similarities without ground truth labels.
- **Deep Learning Model Development:** Created and trained deep neural network models using both PyTorch and TensorFlow in Jupyter notebooks, following best practices for exploring and implementing deep learning techniques.
- **AI Chat-bot Development:** Developed a streaming text-based AI chat-bot for PSU using Svelte, Vercel's AI.sdk, and OpenAI-Edge package, leveraging OpenAI as the platform for robust and responsive interactions.
